{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b8799-b282-4321-ac93-586ed797391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32adf1-56da-46ea-b1a4-b7b340f49733",
   "metadata": {},
   "source": [
    "# loading MNIST dataset\n",
    "MNIST dataset consist of many handwritten digits\n",
    "\n",
    "Datasets loaded by Scikit-Learn generally have a similar dictionary structure including:\n",
    "- A DESCR key describing the dataset\n",
    "- A data key containing an array with one row per instance and one column per feature\n",
    "- A target key containing an array with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327384a-f21e-4f0d-ab50-1e36488eeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto') # as_frame=True will return pandas dataframe\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9d768-98a1-4ae8-8ae8-6c884ebc954c",
   "metadata": {},
   "source": [
    "- There are 70,000 images, and each image has 784 features.\n",
    "- each image is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4b125-7c54-4a5f-ac71-fb920fbb35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e270fa-6c2c-45ee-a701-a8cc5757a0e4",
   "metadata": {},
   "source": [
    "- the values in y is strings. We prefer numbers, so let’s cast y to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2aa4b-cfe9-4c9b-a271-f9d8cbc96a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c9632-b26a-4ed3-a4b4-e4fbeb1781f1",
   "metadata": {},
   "source": [
    "### display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081dc37-af4e-4048-9152-c7acc4bc7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[0]\n",
    "some_digit_image = some_digit.reshape(28,28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708757ef-2636-4649-a074-657edc58d524",
   "metadata": {},
   "source": [
    "### split in train test set\n",
    "MNIST dataset is already divided in train test and shuffeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1541db-d085-4a85-b23c-d187544ec2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13419d-befb-419a-96c9-50bf7b05208b",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29382b3f-c2db-4a49-9da6-f8170fae6767",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) Classifier\n",
    "This clas‐ sifier has the advantage of being capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time (which also makes SGD well suited for online learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cdeacd-b48a-483e-86a9-4de21eec5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits. \n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state = 42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4cba9-62aa-4ffb-8d90-bca9e80c4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a367eb-3eda-452a-a610-a2f77701efcc",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "Evaluating a classifier is often significantly trickier than evaluating a regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ecac7-b205-4186-ab3c-119f00e8d1f6",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662cd20-de6b-48fd-902d-effa26e0e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b0a08-e58d-4d90-a07e-ceb3ca7803e0",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "- The general idea is to count the number of times instances of class A are classified as class B.\n",
    "- Each row in a confusion matrix represents an actual class, while each column repre‐ sents a predicted class. \n",
    "- For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n",
    "- To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1758f96-0b0d-410c-9bd7-459fcfa45dd1",
   "metadata": {},
   "source": [
    "#### Cross val predict\n",
    "Just like the cross_val_score() function, cross_val_predict() performs K-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold.\n",
    "\n",
    "- While cross_val_predict does make multiple predictions during cross-validation, it returns a single array of aggregated predictions for each data point in your input dataset.\n",
    "- Splits the data: It divides the dataset into folds (typically using KFold or StratifiedKFold).\n",
    "- Iterates through folds:\n",
    "  For each fold:\n",
    "    - It trains the model on the remaining folds (excluding the current fold).\n",
    "    - It makes predictions on the data points in the current fold, which are considered as a \"test set\" for that iteration.\n",
    "- Aggregates predictions: After all folds are processed, it combines the individual predictions for each data point into a single array.\n",
    "- Returns the aggregated prediction array: This array has the same length as your original input data, with each element representing the cross-validated prediction for the corresponding data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865b4c1-2e11-4e48-9757-b6d623da0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3) \n",
    "# returns numpy array\n",
    "# default method=\"predict\"\n",
    "# other method: method=\"decision_function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e704350-e56a-4293-9903-c0d4fc2c7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d8751-fd95-46a3-ba0d-bab0df34dd77",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1 score\n",
    "\n",
    "- TN = Element is incorrect and it is predicted as incorrect\n",
    "- FP = False Positive = The element is incorrect but it is predicted as correct\n",
    "- TP = True Positive = The element is correct and it is predicted as correct\n",
    "- FN = False Negative = Element is correct but it is predicted as incorrect\n",
    "\n",
    "                        Predicted\n",
    "  \n",
    "                      Negative   Positive\n",
    "                    \n",
    "                Negative  TN          FP        \n",
    "    Actual\n",
    "\n",
    "                Positive  FN          TP      \n",
    "\n",
    "#### Precision\n",
    "A more concise metric than confusion matrix is Precision\n",
    "- accuracy of the positive pre‐ dictions is called the precision of the classifier\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "#### Recall\n",
    "\n",
    "is the ratio of positive instances that are correctly detected by the classifier.\n",
    "- It is also called sensitivity or true positive rate (TPR)\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "#### F1 Score\n",
    "- The F1 score is the harmonic mean of precision and recall.\n",
    "- Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values.\n",
    "- As a result, the classifier will only get a high F1 score if both recall and precision are high.\n",
    "\n",
    "$$F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99871a7f-8d38-449e-9cc2-4b7ce604c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(precision_score(y_train_5, y_train_pred))\n",
    "print(recall_score(y_train_5, y_train_pred))\n",
    "print(f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb4ebf-5a94-4144-9904-83c576ef5690",
   "metadata": {},
   "source": [
    "### Precision - Recall Tradeoff\n",
    "\n",
    "To understand this tradeoff, let’s look at how the SGDClassifier makes its classification decisions. \n",
    "\n",
    "- For each instance, it computes a score based on a decision function,\n",
    "- and if that score is greater than a threshold, it assigns the instance to the positive class, or else it assigns it to the negative class.\n",
    "- The SGDClassifier uses a threshold equal to 0.\n",
    "- Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s predict() method, you can call its decision_function() method, which returns a score for each instance, and then make predictions based on those scores using any threshold you want\n",
    "- Raising the threshold decreases recall increases precision.\n",
    "- Lowering the threshold increases the recall but decreses the precision.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9dd9b-1cec-46ab-ada7-fbf9dec61717",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = sgd_clf.decision_function([some_digit])\n",
    "print(\"score: \",y_score)\n",
    "\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_score > threshold)\n",
    "print(\"When threshold = 0 \",y_some_digit_pred)\n",
    "\n",
    "threshold = 8000\n",
    "y_some_digit_pred = (y_score > threshold) \n",
    "print(\"When threshold = 8000 \",y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788dc11-aa99-48a0-9375-d82a3213ecc3",
   "metadata": {},
   "source": [
    "This confirms that raising the threshold decreases recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7baed-1f53-41e4-96b4-05ce4af942e4",
   "metadata": {},
   "source": [
    "### Precision Recall Curve \n",
    "\n",
    "Now how do you decide which threshold to use? For this you will first need to get the scores of all instances in the training set using the cross_val_predict() function again, but this time specifying that you want it to return decision scores instead of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ed958-8ec4-4a36-ab8e-75943e5f4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, \n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262eedd-2605-4d9c-8462-4b498fc07dbb",
   "metadata": {},
   "source": [
    "Now with these scores you can compute precision and recall for all possible thresholds using the precision_recall_curve() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de207c82-5c19-45e7-b29e-ec7dddc9dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ca17e-f13d-44bb-814f-14f6edc4b7c8",
   "metadata": {},
   "source": [
    "- Iterates through thresholds: It examines a range of potential decision thresholds, typically starting from the highest score and gradually lowering it.\n",
    "- Classifies predictions at each threshold: For each threshold, it classifies each prediction as TP, FP, TN, or FN based on the y_score and y_true.\n",
    "- Calculates precision and recall: It computes precision (TP / (TP + FP)) and recall (TP / (TP + FN)) at that specific threshold.\n",
    "- Collects values: It appends the precision and recall values obtained for each threshold to separate arrays.\n",
    "- Returns arrays: It returns two arrays: one containing precision values for each threshold, and the other containing recall values for each threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccc19c-74af-405e-a7ee-d8d7bcecb6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): \n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\") \n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Blue: Precision, Green: Recall\")\n",
    "\n",
    "    plt.xlim([-30000, 30000])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "\n",
    "    plt.grid(True)\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d021b-4536-4880-8d54-79175f52d042",
   "metadata": {},
   "source": [
    "#### Precision vs Recall curve\n",
    "Another way to select a good precision/recall tradeoff is to plot precision directly against recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09fff6-5d73-4cbe-b9c6-16472c9bd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Recall\")\n",
    "plt.plot(recalls, precisions, \"b--\", label=\"Precision\") \n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e688636-50de-4195-b28c-f8c0a72d5b4a",
   "metadata": {},
   "source": [
    "- We can see precision starts to fall sharply after certian threshold\n",
    "- You will probably want to select a precision/recall tradeoff just before that drop.\n",
    "- the choice depends on your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887345a-4d2d-4f59-8515-cb1e74de7163",
   "metadata": {},
   "source": [
    "So let’s suppose you decide to aim for 90% precision. You look up the first plot and find that you need to use a threshold of about 8,000. To be more precise you can search for the lowest threshold that gives you at least 90% precision (np.argmax() will give us the first index of the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e425c-2422-410a-99bd-098308f9bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064bfbf-637c-42bf-a4c0-47ac913c3b90",
   "metadata": {},
   "source": [
    "To make predictions (on the training set for now), instead of calling the classifier’s\n",
    "predict() method, you can just run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3767f4-8856-4356-84c8-d77344f6b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fc054-62f2-4264-a3f5-ab27ccccc713",
   "metadata": {},
   "source": [
    "Check the predictions's precision and recall value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c0812-2266-4193-9f44-b4907cc42d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_train_5, y_train_pred_90))\n",
    "print(recall_score(y_train_5, y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f92ec9-c01c-4bab-b226-0a55d02104da",
   "metadata": {},
   "source": [
    "## receiver operating characteristic (ROC) curve\n",
    "- common tool used with binary classifiers\n",
    "- ROC curve plots the true positive rate (another name for recall) against the false positive rate.\n",
    "- The FPR is the ratio of negative instances that are incorrectly classified as positive.\n",
    "- It is equal to one minus the true negative rate, which is the ratio of negative instances that are correctly classified as negative.\n",
    "- The TNR is also called specificity.\n",
    "- Hence the ROC curve plots sensitivity (recall) versus 1 – specificity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe04e596-ca9b-4ac5-96bb-7543e7a99c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None): \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label) \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    # Add axes labels and title:\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c94b1-9f61-4295-9e9f-f0340ce525fa",
   "metadata": {},
   "source": [
    "The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A per‐ fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7935b71-2595-4cdf-af11-847b9d8e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e77ea7-ad27-4126-a9f4-8314bebbd958",
   "metadata": {},
   "source": [
    "### what to use PR vs ROC\n",
    "\n",
    "Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives, and the ROC curve otherwise. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement (the curve could be closer to the top- right corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc2976-cd0a-421d-b61c-1669e92e98be",
   "metadata": {},
   "source": [
    "# Conclusion for binary classification\n",
    "\n",
    "Hopefully you now know how to train binary classifiers, choose the appropriate met‐ ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves and ROC AUC scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836352d7-eebf-48d5-ac02-0b7c2ffa5b83",
   "metadata": {},
   "source": [
    "# Multi Class Classifiaction (multinomial classifiers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d099c65-be36-4dd2-9174-99517d6b88e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.11 kernel",
   "language": "python",
   "name": "myjupyterkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
